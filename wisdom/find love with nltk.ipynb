{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find love with NLTK <3\n",
    "\n",
    "- mail: tamkien@cri-paris.org\n",
    "- twitter: @wekeypedia\n",
    "- website: http://github.com/wekeypedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While looking at the content of wikipedia diff of [Love](http://en.wikipedia.org/w/Love), I was surprised by the number and the specificity of \"vandalisms\" on it. It make this page looking more like a high school table full of markings or a carved tree in the park. I find it more funny that deterioring the encyclopedia, a sign of the exteriority of this object of knowledge far from being a pure ideal space of wisdom. Luckily a lot of bots and users are working hard to delete and make those signs of love invisible. I just wanted to see what are the proportions of the phenomenom. In my mind, using [nltk](http://nltk.org) would make it very easy and lower the supervision to a minimum and would also be a good excercice to start using that library.\n",
    "\n",
    "First we are going to load the [diff from a cache](https://github.com/WeKeyPedia/notebooks/tree/master/wisdom/data/Love) and also used the [wekeypedia python library](http://github.com/wekeypedia/toolkit-python) to skip the data acquision and parsing part. to jump into the fun of basic usage of nltk tree tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common libraries loaded\n"
     ]
    }
   ],
   "source": [
    "%run \"libraries.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revisions: 6324\n"
     ]
    }
   ],
   "source": [
    "def from_file(name):\n",
    "  diff_txt = \"\"\n",
    "\n",
    "  with codecs.open(name, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "  return data\n",
    "\n",
    "def list_revisions(page):\n",
    "  return os.listdir(\"data/%s\" % (page))\n",
    "\n",
    "def load_revisions(s):\n",
    "  revisions = defaultdict(dict)\n",
    "  \n",
    "  p = wekeypedia.WikipediaPage(s)\n",
    "  \n",
    "  revisions_list = list_revisions(s)\n",
    "  revisions_list = map(lambda revid: revid.split(\".\")[0], revisions_list)\n",
    "  \n",
    "  revisions = { revid : from_file(\"data/%s/%s.json\" % (s, revid)) for revid in revisions_list }\n",
    "  \n",
    "  return revisions\n",
    "\n",
    "revisions = load_revisions(\"Love\")\n",
    "\n",
    "print \"revisions: %s\" % len(revisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page = wekeypedia.WikipediaPage(\"Love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## detect love\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎♥︎\n",
      "684\n"
     ]
    }
   ],
   "source": [
    "def detect_love(revid):\n",
    "  result = {\n",
    "    \"revid\": revid,\n",
    "    \"love\": [],\n",
    "    \"plusminus\": {}\n",
    "  }\n",
    "  \n",
    "  diff = revisions[revid][\"diff\"][\"*\"]\n",
    "  diff = page.extract_plusminus(diff)\n",
    "  \n",
    "  result[\"plusminus\"] = diff\n",
    "  \n",
    "  rev_index = revisions.keys()\n",
    "  print \"\\rrevision: %s/%s\" % ( rev_index.index(revid), len(rev_index)),\n",
    "  \n",
    "  for sentence in diff[\"added\"]:\n",
    "    pos_tags = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "    if len([ t for t in pos_tags if t[1] == \"NNP\" \\\n",
    "            and not(\"love\" in t[0].lower())]) >= 2 \\\n",
    "            and 2 < len(pos_tags) < 20 \\\n",
    "            and \"love\" in sentence.lower():\n",
    "      \n",
    "      result[\"love\"].append(sentence)\n",
    "      print \" ♥︎\",\n",
    "\n",
    "  return result\n",
    "\n",
    "# revlist = random.sample(revisions.keys(), 100)\n",
    "revlist = revisions.keys()\n",
    "\n",
    "results = [ detect_love(revid) for revid in revlist ]\n",
    "print \"\\r \",\n",
    "\n",
    "love = [ s for s in results if len(s[\"love\"]) > 0 ]\n",
    "\n",
    "print \"♥︎\" * len(love)\n",
    "print len(love)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning and saving the result for manual cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'love' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-47666cbf3e71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfinal_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlove\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"love\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'love' is not defined"
     ]
    }
   ],
   "source": [
    "final_result = []\n",
    "\n",
    "for l in love:\n",
    "  for s in l[\"love\"]:\n",
    "    if not(\"[\" in s) and not(\"*\" in s) and not(\"==\" in s):\n",
    "      final_result.append( [ l[\"revid\"], s ] )\n",
    "\n",
    "final_result = pd.DataFrame(final_result)\n",
    "final_result.columns = [\"revision id\", \"sentence\"]\n",
    "final_result.head()\n",
    "\n",
    "print len(final_result)\n",
    "\n",
    "final_result.to_csv(\"data/find_love.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point there is still ~70 false positives and no counting of false negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual cleaning of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n"
     ]
    }
   ],
   "source": [
    "final_result = pd.DataFrame.from_csv(\"data/find_love-checked.csv\", encoding=\"utf-8\")\n",
    "final_result = final_result.drop(final_result[final_result[\"false positive\"] == 1].index)\n",
    "\n",
    "print len(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## insert the love back into the page\n",
    "\n",
    "In order to write back all the marks of affection into the wikipedia page, we first need to extract where it was added first. The following implementation is not entirely accurate since its retrieve the line number in the previous version of the page and not the current one. Since it is only for play purpose, we will not be bother by that detail.\n",
    "\n",
    "Retrieving the line number is relatively easy, we just need to extract `<td class=\"diff-lineno\">` tags. For more information, you can still consult the previous notebook: [parsing wikipedia diff](parsing%20wikipedia%20diff.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_line_diff(revid):\n",
    "  line = 0\n",
    "  \n",
    "  content = revisions[u\"\"+str(revid)][\"diff\"][\"*\"]\n",
    "  html = BeautifulSoup(content, 'html.parser')\n",
    "  \n",
    "  # <td colspan=\"2\" class=\"diff-lineno\">\n",
    "  line = html.find(\"td\", class_=\"diff-lineno\").get_text()\n",
    "  line = line.split(\" \")[1]\n",
    "  line = line[0:-1]\n",
    "  \n",
    "  return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Preview</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe src=outputs/findlove.html width=700 height=350></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# content = page.get_revision()\n",
    "content = BeautifulSoup(page.get_revision(), 'html.parser')\n",
    "\n",
    "for l in final_result.index:\n",
    "  line = get_line_diff(l)\n",
    "  tag = u\"<span class=\\\"love\\\" data-revision_id=\\\"%s\\\">♥︎</span>\" % (l)\n",
    "  \n",
    "  content.insert(int(line), BeautifulSoup(tag, 'html.parser'))\n",
    "\n",
    "with codecs.open(\"outputs/findlove.html\", \"w\", \"utf-8-sig\") as f:\n",
    "  f.write(content.prettify())\n",
    "  f.close()\n",
    "  \n",
    "display(HTML(\"<h1>Preview</h1>\"))\n",
    "display(HTML(\"<iframe src=outputs/findlove.html width=700 height=350></iframe>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the [page with a better CSS and link that redirect to the corresponding revision](http://wekeypedia.net/findlove)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extra: computing the top love"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you (13)\n",
      "and (12)\n",
      "forever (10)\n",
      "u (10)\n",
      "example (9)\n",
      "god (8)\n",
      "john (8)\n",
      "the (8)\n",
      "sarah (7)\n",
      "more (7)\n",
      "yu (7)\n",
      "laura (6)\n",
      "simon (6)\n",
      "greek (6)\n",
      "x (6)\n",
      "anthony (6)\n",
      "danielle (5)\n",
      "helen (5)\n",
      "him (5)\n",
      "amanda (5)\n",
      "sex (5)\n",
      "josh (5)\n",
      "fisher (5)\n",
      "ali (4)\n",
      "emily (4)\n",
      "to (4)\n",
      "baby (4)\n",
      "my (4)\n",
      "fuck (4)\n",
      "a (4)\n",
      "alex (4)\n",
      "derek (4)\n",
      "nature (4)\n",
      "andrew (4)\n",
      "chemistry (4)\n",
      "true (4)\n",
      "nate (4)\n",
      "steph (4)\n",
      "kathleen (4)\n",
      "romantic (4)\n",
      "jessy (4)\n",
      "michael (3)\n",
      "ever (3)\n",
      "christian (3)\n",
      "lee (3)\n",
      "avery (3)\n",
      "mel (3)\n",
      "drew (3)\n",
      "boy (3)\n",
      "rory (3)\n",
      "chang (3)\n",
      "kristina (3)\n",
      "from (3)\n",
      "call (3)\n",
      "babe (3)\n",
      "/ref (3)\n",
      "help (3)\n",
      "stephen (3)\n",
      "hall (3)\n",
      "nick (3)\n",
      "mandy (3)\n",
      "muhammad (3)\n",
      "need (3)\n",
      "dom (3)\n",
      "abby (3)\n",
      "best (3)\n",
      "whole (3)\n",
      "tina (3)\n",
      "life (3)\n",
      "dylan (3)\n",
      "i (3)\n",
      "kelly (3)\n",
      "world (3)\n",
      "like (3)\n",
      "always (3)\n",
      "eric (3)\n",
      "much (3)\n",
      "jason (2)\n",
      "augustine (2)\n",
      "hilda (2)\n",
      "agron (2)\n",
      "go (2)\n",
      "hate (2)\n",
      "gabrielle (2)\n",
      "jessie (2)\n",
      "hannah (2)\n",
      "joey (2)\n",
      "cameron (2)\n",
      "victor (2)\n",
      "men (2)\n",
      "xharah (2)\n",
      "box (2)\n",
      "bible (2)\n",
      "nguyen (2)\n",
      "taub (2)\n",
      "nina (2)\n",
      "akil (2)\n",
      "alyssa (2)\n",
      "crazy (2)\n",
      "dolphin (2)\n",
      "poetry (2)\n",
      "schneider (2)\n",
      "over (2)\n",
      "blake (2)\n",
      "curtis (2)\n",
      "now (2)\n",
      "james (2)\n",
      "lucy (2)\n",
      "page=169 (2)\n",
      "girl (2)\n",
      "shannon (2)\n",
      "day (2)\n",
      "xxxmarc (2)\n",
      "jessica (2)\n",
      "lizzie (2)\n",
      "nobbs (2)\n",
      "david (2)\n",
      "cain (2)\n",
      "constantly (2)\n",
      "yair (2)\n",
      "ian (2)\n",
      "jummana (2)\n",
      "carole.. (2)\n",
      "saint (2)\n",
      "alan (2)\n",
      "hillary (2)\n",
      "jordan (2)\n",
      "constanza (2)\n",
      "roberts (2)\n",
      "lover (2)\n",
      "brandon (2)\n",
      "gill (2)\n",
      "geosits (2)\n",
      "m (2)\n",
      "chester (2)\n",
      "=d (2)\n",
      "duncan (2)\n",
      "'erota (2)\n",
      "get (2)\n",
      "between (2)\n",
      "christopher (2)\n",
      "emo (2)\n",
      "daugherty (2)\n",
      "pants (2)\n",
      "matveet (2)\n",
      "amer (2)\n",
      "c (2)\n",
      "amazing (2)\n",
      "tarek (2)\n",
      "marc (2)\n",
      "bianchi (2)\n",
      "vince (2)\n",
      "andrea (2)\n",
      "will (2)\n",
      "it (2)\n",
      "kortney (2)\n",
      "brenna (2)\n",
      "madly (2)\n",
      "harris (2)\n",
      "albie (2)\n",
      "kyle (2)\n",
      "lauren (2)\n",
      "bryce (2)\n",
      "courtney (2)\n",
      "jake (2)\n",
      "armbruster (2)\n",
      "emma (2)\n",
      "rachael (2)\n",
      "dean (2)\n",
      "fucking (2)\n",
      "kenrick (2)\n",
      "brent (2)\n",
      "cooper (2)\n",
      "bobbi (2)\n",
      "by (2)\n",
      "on (2)\n",
      "brittany (2)\n",
      "hot (2)\n",
      "beth (2)\n",
      "happy (2)\n",
      "alexa (2)\n",
      "farina (2)\n",
      "caitlin (2)\n",
      "manion (2)\n",
      "an (2)\n",
      "nico (2)\n",
      "bianca (2)\n",
      "chris (2)\n",
      "natalie (2)\n",
      "all (1)\n",
      "cynthia (1)\n",
      "known (1)\n",
      "colleen (1)\n",
      "kate (1)\n",
      "cheney (1)\n",
      "edward (1)\n",
      "sona (1)\n",
      "word (1)\n",
      "mandi (1)\n",
      "joel (1)\n",
      "t. (1)\n",
      "soooo (1)\n",
      "dix (1)\n",
      "lisa (1)\n",
      "dhaliwal (1)\n",
      "carly (1)\n",
      "sign (1)\n",
      "poll (1)\n",
      "n (1)\n",
      "what (1)\n",
      "chicago (1)\n",
      "+ (1)\n",
      "sooooooo (1)\n",
      "goes (1)\n",
      "angelone (1)\n",
      "petra (1)\n",
      "edited (1)\n",
      "bo (1)\n",
      "zach (1)\n",
      "chinar (1)\n",
      "darcie (1)\n",
      "strong (1)\n",
      "institute (1)\n",
      "k (1)\n",
      "experience (1)\n",
      "belford (1)\n",
      "bnmike (1)\n",
      "male (1)\n",
      "alexandria (1)\n",
      "barkman (1)\n",
      "sgj (1)\n",
      "bre (1)\n",
      "huzzah (1)\n",
      "would (1)\n",
      "m. (1)\n",
      "sutphin (1)\n",
      "jonathan (1)\n",
      "tell (1)\n",
      "jordyn (1)\n",
      "biatch (1)\n",
      "charity (1)\n",
      "ma (1)\n",
      "this (1)\n",
      "sucks (1)\n",
      "trang (1)\n",
      "whitten (1)\n",
      "rohan (1)\n",
      "cabeza (1)\n",
      "v (1)\n",
      "pleasurable (1)\n",
      "heart (1)\n",
      "waaaay (1)\n",
      "@ (1)\n",
      "genetailia (1)\n",
      "mohan (1)\n",
      "browne (1)\n",
      "chemical (1)\n",
      "ilir (1)\n",
      "pure (1)\n",
      "dannielle (1)\n",
      "mccoy (1)\n",
      "beach (1)\n",
      "aniket (1)\n",
      "''i (1)\n",
      "pat (1)\n",
      "nympho (1)\n",
      "man (1)\n",
      "rodrigue (1)\n",
      "jodie (1)\n",
      "susan (1)\n",
      "chapman (1)\n",
      "kaity (1)\n",
      "so (1)\n",
      "grande (1)\n",
      "okcupid.com (1)\n",
      "hsnnu (1)\n",
      "singaporean (1)\n",
      "sanchez (1)\n",
      "september (1)\n",
      "soon (1)\n",
      "murray (1)\n",
      "shiels (1)\n",
      "scott (1)\n",
      "/a (1)\n",
      "mabel (1)\n",
      "sharmin (1)\n",
      "bulcock (1)\n",
      "ariam (1)\n",
      "mizzie (1)\n",
      "brook (1)\n",
      "piny (1)\n",
      "hoang (1)\n",
      "romeo (1)\n",
      "alexander (1)\n",
      "eugene (1)\n",
      "tattvasmasi (1)\n",
      "thei (1)\n",
      "olivia (1)\n",
      "adri (1)\n",
      "name (1)\n",
      "trac (1)\n",
      "betty (1)\n",
      "urieeeeeeeeeeeee (1)\n",
      "mean (1)\n",
      "el (1)\n",
      "neil (1)\n",
      "diana (1)\n",
      "hutsoall (1)\n",
      "steven (1)\n",
      "avisha (1)\n",
      "our (1)\n",
      "sexual (1)\n",
      "out (1)\n",
      "flaccid (1)\n",
      "daniel (1)\n",
      "she (1)\n",
      "ass (1)\n",
      "samya (1)\n",
      "jill (1)\n",
      "caro (1)\n",
      "keara (1)\n",
      "card (1)\n",
      "definition (1)\n",
      "kristyn (1)\n",
      "kirsty (1)\n",
      "language (1)\n",
      "thing (1)\n",
      "lindsey (1)\n",
      "austin (1)\n",
      "south (1)\n",
      "omgzorz (1)\n",
      "rachel (1)\n",
      "kaela (1)\n",
      "valentine (1)\n",
      "president (1)\n",
      "george (1)\n",
      "koh (1)\n",
      "little (1)\n",
      "garrick (1)\n",
      "anyone (1)\n",
      "bitz (1)\n",
      "tom (1)\n",
      "jen (1)\n",
      "hurt. (1)\n",
      "caitlyn (1)\n",
      "dolan (1)\n",
      "giselle (1)\n",
      "jamie (1)\n",
      "huw (1)\n",
      "shirley (1)\n",
      "vishal (1)\n",
      "than (1)\n",
      "ben (1)\n",
      "arun (1)\n",
      "sophie (1)\n",
      "feeling (1)\n",
      "stacey (1)\n",
      "are (1)\n",
      "fvnshdlhvb (1)\n",
      "sam (1)\n",
      "craig (1)\n",
      "love|url=http (1)\n",
      "snet-puss (1)\n",
      "ant (1)\n",
      "duvall (1)\n",
      "nikolay (1)\n",
      "have (1)\n",
      "goodie (1)\n",
      "well (1)\n",
      "boners (1)\n",
      "sherri (1)\n",
      "gilburt (1)\n",
      "connell (1)\n",
      "note (1)\n",
      "shamoon (1)\n",
      "bourgeois (1)\n",
      "green (1)\n",
      "dannille (1)\n",
      "brendon (1)\n",
      "who (1)\n",
      "skinner (1)\n",
      "nadine (1)\n",
      "marie (1)\n",
      "michelle (1)\n",
      "yang (1)\n",
      "tattvamasi (1)\n",
      "jocelyn (1)\n",
      "dot (1)\n",
      "thomas (1)\n",
      "christyy (1)\n",
      "joshi (1)\n",
      "kapel (1)\n",
      "naomi (1)\n",
      "limerence (1)\n",
      "scotti (1)\n",
      "it’s (1)\n",
      "babyyyyyyy (1)\n",
      "randy (1)\n",
      "black (1)\n",
      "rich (1)\n",
      "grutta (1)\n",
      "monica (1)\n",
      "dj (1)\n",
      "mortally (1)\n",
      "agapo (1)\n",
      "dustin (1)\n",
      "sanaz (1)\n",
      "schembri (1)\n",
      "yahoo.co.uk (1)\n",
      "joy (1)\n",
      "travis (1)\n",
      "leslie (1)\n",
      "sean (1)\n",
      "davyion (1)\n",
      "ari (1)\n",
      "juliet (1)\n",
      "chogan (1)\n",
      "temptation (1)\n",
      "hajir (1)\n",
      "stepehen (1)\n",
      "melissa (1)\n",
      "horrible (1)\n",
      "mum (1)\n",
      "latin (1)\n",
      "hazim (1)\n",
      "key (1)\n",
      "hurts (1)\n",
      "article (1)\n",
      "reaction (1)\n",
      "mindy (1)\n",
      "olga (1)\n",
      "keaton (1)\n",
      "s (1)\n",
      "annie (1)\n",
      "whorton (1)\n",
      "lovez (1)\n",
      "jeff (1)\n",
      "imadlak (1)\n",
      "leon (1)\n",
      "taylor (1)\n",
      "church (1)\n",
      "kayte.. (1)\n",
      "nicole (1)\n",
      "smith (1)\n",
      "robbie (1)\n",
      "mark (1)\n",
      "empty (1)\n",
      "sacha (1)\n",
      "website (1)\n",
      "gay (1)\n",
      "tomei (1)\n",
      "kandice (1)\n",
      "remy (1)\n",
      "undescribable (1)\n",
      "air (1)\n",
      "phuck (1)\n",
      "cox (1)\n",
      "eii (1)\n",
      "gupita (1)\n",
      "lacey (1)\n",
      "mitchell (1)\n",
      "robin (1)\n",
      "-justine (1)\n",
      "hewitt (1)\n",
      "lust (1)\n",
      "galipchak (1)\n",
      "im (1)\n",
      "heels (1)\n",
      "keerthana (1)\n",
      "whore (1)\n",
      "babii (1)\n",
      "mclaury (1)\n",
      "any (1)\n",
      "matthew (1)\n",
      "ruth (1)\n",
      "bunny (1)\n",
      "qaz3d (1)\n",
      "phi (1)\n",
      "goodwin (1)\n",
      "ella (1)\n",
      "blair (1)\n",
      "stacy (1)\n",
      "barber (1)\n",
      "kim (1)\n",
      "tung (1)\n",
      "navas (1)\n",
      "phu (1)\n",
      "ashley (1)\n",
      "com (1)\n",
      "obviously (1)\n",
      "kyran (1)\n",
      "lim (1)\n",
      "harney (1)\n",
      "ajitpal (1)\n",
      "..i (1)\n",
      "being (1)\n",
      "human (1)\n",
      "paul (1)\n",
      "seductively (1)\n",
      "cocks (1)\n",
      "moretti (1)\n",
      "jose (1)\n",
      "death (1)\n",
      "bryn (1)\n",
      "tim (1)\n",
      "rawrrrrrrrrrrrrrrrrrrrrrrrrr (1)\n",
      "rose (1)\n",
      "nikki (1)\n",
      "bullshit (1)\n",
      "thermodynamics (1)\n",
      "maisey (1)\n",
      "real (1)\n",
      "aniika (1)\n",
      "monique (1)\n",
      "amy (1)\n",
      "bastards (1)\n",
      "holly (1)\n",
      "rebecca (1)\n",
      "adams (1)\n",
      "christina (1)\n",
      "christine (1)\n",
      "kennedy (1)\n",
      "danelle (1)\n",
      "patrick (1)\n",
      "ciardi (1)\n",
      "clay (1)\n",
      "thrower (1)\n",
      "hung (1)\n",
      "alexandra (1)\n",
      "jaimee (1)\n",
      "shea (1)\n",
      "shev (1)\n",
      "vsd (1)\n",
      "peter (1)\n",
      "scale (1)\n",
      "for (1)\n",
      "soto (1)\n",
      "gaskell (1)\n",
      "crossley (1)\n",
      "be (1)\n",
      "bb (1)\n",
      "a. (1)\n",
      "anything (1)\n",
      "of (1)\n",
      "hutchinson (1)\n",
      "amber (1)\n",
      "bowny (1)\n",
      "johns (1)\n",
      "“to (1)\n",
      "tajkowski (1)\n",
      "wade (1)\n",
      "contreras (1)\n",
      "your (1)\n",
      "katie (1)\n",
      "her (1)\n",
      "loh (1)\n",
      "there (1)\n",
      "/math (1)\n",
      "joseph (1)\n",
      "assholers (1)\n",
      "was (1)\n",
      "head (1)\n",
      "shane (1)\n",
      "bitches (1)\n",
      "melanie (1)\n",
      "hi (1)\n",
      "'bold (1)\n",
      "line (1)\n",
      "botts (1)\n",
      "he (1)\n",
      "beckie (1)\n",
      "fanella (1)\n",
      "faggot (1)\n",
      "jenny (1)\n",
      "nazish.. (1)\n",
      "felix (1)\n",
      "chan (1)\n",
      "mike (1)\n",
      "erin (1)\n",
      "walters (1)\n",
      "sargeant (1)\n",
      "heather (1)\n",
      "no (1)\n",
      "djrttu (1)\n",
      "sharif (1)\n",
      "parikh (1)\n",
      "nat (1)\n",
      "love_life_advice (1)\n",
      "gemma (1)\n",
      "adam (1)\n",
      "shaun (1)\n",
      "walton (1)\n",
      "bonding (1)\n",
      "evan (1)\n",
      "jeremy (1)\n",
      "'i (1)\n",
      "jones (1)\n",
      "casey (1)\n",
      "vice (1)\n",
      "together (1)\n",
      "corey (1)\n",
      "nelson (1)\n",
      "paige (1)\n"
     ]
    }
   ],
   "source": [
    "names = defaultdict(int)\n",
    "\n",
    "for sentence in list(final_result[\"sentence\"]):\n",
    "  pos_tags = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "  \n",
    "  for w in pos_tags:\n",
    "    if w[1] == \"NNP\":\n",
    "      names[w[0].lower()] += 1\n",
    "\n",
    "names = sorted(names.items(), key=lambda x: -x[1])\n",
    "\n",
    "ignore = [ \"love\", \"loves\", \"is\", \"<\", \">\", \"in\", \"with\", \"}\", \"{\", \"(\", \")\", \"=\", \"[\", \"]\"]\n",
    "names = [ n for n in names if not(n[0] in ignore) ]\n",
    "#names = { x[0]: x[1] for x in names if x[1] not in ignore }\n",
    "\n",
    "for n in names:\n",
    "  print \"%s (%s)\" % (n[0], n[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
